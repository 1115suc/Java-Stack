# MinIO 大文件上传优化

## 策略一：MinIO 分片上传

### 1.基础分片上传实现

```java
@Service
@RequiredArgsConstructor
@Slf4j
public class MinioUploadService {
    private MinioClient minioClient;
    private MinioConfigurationProperties minioConfig;

    // 默认分片大小: 10MB
    private static final long PART_SIZE = 10 * 1024 * 1024;
    
    public String uploadLargeFile(String prefix, String filename, 
                                   InputStream inputStream, long fileSize) {
        String filePath = buildFilePath(prefix, filename);
        
        try {
            // 使用ObjectWriteResponse获取更多信息
            ObjectWriteResponse response = minioClient.putObject(
                PutObjectArgs.builder()
                    .bucket(minioConfig.getBucket())
                    .object(filePath)
                    .contentType("video/mp4")
                    // ✅ 正确设置: 文件大小, 分片大小
                    .stream(inputStream, fileSize, PART_SIZE)
                    .build()
            );
            
            log.info("Upload success, etag: {}", response.etag());
            return buildAccessUrl(filePath);
            
        } catch (Exception e) {
            log.error("Upload failed", e);
            throw new RuntimeException("Upload failed", e);
        }
    }
}
```

---

## 策略二：多线程并行分片上传

### 1. 分片上传管理器

```java
@Service
@Slf4j
@RequiredArgsConstructor
public class MultipartUploadService {
    private MinioClient minioClient;
    private MinioConfigurationProperties minioConfig;
    
    // 分片大小: 5MB (MinIO最小分片大小为5MB)
    private static final long PART_SIZE = 5 * 1024 * 1024;
    
    // 并行上传线程数
    private static final int PARALLEL_THREADS = 4;
    
    private final ExecutorService uploadExecutor = Executors.newFixedThreadPool(
        PARALLEL_THREADS,
        new ThreadFactoryBuilder().setNameFormat("minio-upload-%d").build()
    );

    /**
     * 并行分片上传
     */
    public String parallelUpload(String prefix, String filename, 
                                  File file) throws Exception {
        String objectName = buildFilePath(prefix, filename);
        String bucket = minioConfig.getBucket();
        long fileSize = file.length();
        
        // 计算分片数量
        int partCount = (int) Math.ceil((double) fileSize / PART_SIZE);
        log.info("File size: {}MB, Part count: {}", fileSize / 1024 / 1024, partCount);
        
        // 1. 初始化分片上传
        String uploadId = initMultipartUpload(bucket, objectName);
        log.info("Multipart upload initialized, uploadId: {}", uploadId);
        
        try {
            // 2. 并行上传所有分片
            List<CompletableFuture<PartInfo>> futures = new ArrayList<>();
            
            for (int partNumber = 1; partNumber <= partCount; partNumber++) {
                final int currentPart = partNumber;
                long offset = (long) (partNumber - 1) * PART_SIZE;
                long size = Math.min(PART_SIZE, fileSize - offset);
                
                CompletableFuture<PartInfo> future = CompletableFuture.supplyAsync(() -> 
                    uploadPart(bucket, objectName, uploadId, currentPart, file, offset, size),
                    uploadExecutor
                );
                futures.add(future);
            }
            
            // 3. 等待所有分片上传完成
            List<PartInfo> parts = futures.stream()
                .map(CompletableFuture::join)
                .sorted(Comparator.comparingInt(PartInfo::getPartNumber))
                .collect(Collectors.toList());
            
            // 4. 完成分片上传
            completeMultipartUpload(bucket, objectName, uploadId, parts);
            
            log.info("Parallel upload completed: {}", objectName);
            return buildAccessUrl(objectName);
            
        } catch (Exception e) {
            // 上传失败，取消分片上传
            abortMultipartUpload(bucket, objectName, uploadId);
            throw e;
        }
    }
    
    /**
     * 初始化分片上传
     */
    private String initMultipartUpload(String bucket, String objectName) throws Exception {
        CreateMultipartUploadResponse response = minioClient.createMultipartUpload(
            CreateMultipartUploadArgs.builder()
                .bucket(bucket)
                .object(objectName)
                .contentType("video/mp4")
                .build()
        );
        return response.result().uploadId();
    }
    
    /**
     * 上传单个分片
     */
    private PartInfo uploadPart(String bucket, String objectName, String uploadId,
                                 int partNumber, File file, long offset, long size) {
        try (RandomAccessFile raf = new RandomAccessFile(file, "r")) {
            raf.seek(offset);
            byte[] buffer = new byte[(int) size];
            raf.readFully(buffer);
            
            UploadPartResponse response = minioClient.uploadPart(
                UploadPartArgs.builder()
                    .bucket(bucket)
                    .object(objectName)
                    .uploadId(uploadId)
                    .partNumber(partNumber)
                    .stream(new ByteArrayInputStream(buffer), size, -1)
                    .build()
            );
            
            log.debug("Part {} uploaded, etag: {}", partNumber, response.etag());
            return new PartInfo(partNumber, response.etag());
            
        } catch (Exception e) {
            throw new RuntimeException("Failed to upload part " + partNumber, e);
        }
    }
    
    /**
     * 完成分片上传
     */
    private void completeMultipartUpload(String bucket, String objectName, 
                                          String uploadId, List<PartInfo> parts) throws Exception {
        Part[] partsArray = parts.stream()
            .map(p -> new Part(p.getPartNumber(), p.getEtag()))
            .toArray(Part[]::new);
            
        minioClient.completeMultipartUpload(
            CompleteMultipartUploadArgs.builder()
                .bucket(bucket)
                .object(objectName)
                .uploadId(uploadId)
                .parts(partsArray)
                .build()
        );
    }
    
    /**
     * 取消分片上传
     */
    private void abortMultipartUpload(String bucket, String objectName, 
                                       String uploadId) {
        try {
            minioClient.abortMultipartUpload(
                AbortMultipartUploadArgs.builder()
                    .bucket(bucket)
                    .object(objectName)
                    .uploadId(uploadId)
                    .build()
            );
        } catch (Exception e) {
            log.error("Failed to abort multipart upload", e);
        }
    }
    
    @Data
    @AllArgsConstructor
    static class PartInfo {
        private int partNumber;
        private String etag;
    }
}
```

--- 

## 策略三：断点续传实现

### 1. 上传进度持久化

```java
@Service
@RequiredArgsConstructor
@Slf4j
public class ResumableUploadService {
    private MinioClient minioClient;
    private RedisTemplate<String, Object> redisTemplate;
    
    private static final String UPLOAD_PROGRESS_KEY = "minio:upload:progress:";
    private static final long PART_SIZE = 5 * 1024 * 1024;

    /**
     * 断点续传主方法
     */
    public String resumableUpload(String fileId, String prefix, 
                                   String filename, File file) throws Exception {
        String objectName = buildFilePath(prefix, filename);
        String bucket = minioConfig.getBucket();
        
        // 1. 检查是否有未完成的上传
        UploadProgress progress = getUploadProgress(fileId);
        
        String uploadId;
        Set<Integer> uploadedParts;
        
        if (progress != null && !isUploadExpired(progress)) {
            // 继续之前的上传
            uploadId = progress.getUploadId();
            uploadedParts = progress.getUploadedParts();
            log.info("Resuming upload, uploadId: {}, uploaded parts: {}", 
                     uploadId, uploadedParts.size());
        } else {
            // 开始新的上传
            uploadId = initMultipartUpload(bucket, objectName);
            uploadedParts = new HashSet<>();
            log.info("Starting new upload, uploadId: {}", uploadId);
        }
        
        try {
            long fileSize = file.length();
            int totalParts = (int) Math.ceil((double) fileSize / PART_SIZE);
            List<PartInfo> allParts = new ArrayList<>();
            
            // 2. 上传未完成的分片
            for (int partNumber = 1; partNumber <= totalParts; partNumber++) {
                if (uploadedParts.contains(partNumber)) {
                    // 分片已上传，跳过
                    log.debug("Part {} already uploaded, skipping", partNumber);
                    continue;
                }
                
                long offset = (long) (partNumber - 1) * PART_SIZE;
                long size = Math.min(PART_SIZE, fileSize - offset);
                
                // 上传分片
                PartInfo partInfo = uploadPart(bucket, objectName, uploadId, 
                                               partNumber, file, offset, size);
                allParts.add(partInfo);
                
                // 更新进度
                uploadedParts.add(partNumber);
                saveUploadProgress(fileId, new UploadProgress(
                    uploadId, objectName, uploadedParts, System.currentTimeMillis()
                ));
                
                // 计算并回调进度
                double percent = (double) uploadedParts.size() / totalParts * 100;
                log.info("Upload progress: {}/{} ({:.2f}%)", 
                         uploadedParts.size(), totalParts, percent);
            }
            
            // 3. 完成上传
            completeMultipartUpload(bucket, objectName, uploadId, allParts);
            
            // 4. 清理进度记录
            deleteUploadProgress(fileId);
            
            return buildAccessUrl(objectName);
            
        } catch (Exception e) {
            log.error("Upload failed, progress saved for resumption", e);
            throw e;
        }
    }
    
    /**
     * 保存上传进度
     */
    private void saveUploadProgress(String fileId, UploadProgress progress) {
        redisTemplate.opsForValue().set(
            UPLOAD_PROGRESS_KEY + fileId,
            progress,
            24, TimeUnit.HOURS  // 24小时过期
        );
    }
    
    /**
     * 获取上传进度
     */
    private UploadProgress getUploadProgress(String fileId) {
        return (UploadProgress) redisTemplate.opsForValue()
            .get(UPLOAD_PROGRESS_KEY + fileId);
    }
    
    @Data
    @AllArgsConstructor
    static class UploadProgress implements Serializable {
        private String uploadId;
        private String objectName;
        private Set<Integer> uploadedParts;
        private long lastUpdateTime;
    }
}
```

--- 

## 策略四：前端分片 + 秒传

### 1. 后端接口设计

```java
@RestController
@RequestMapping("/api/upload")
@Slf4j
public class ChunkUploadController {

    @Autowired
    private ChunkUploadService chunkUploadService;

    /**
     * 检查文件是否已存在(秒传)
     */
    @GetMapping("/check")
    public ResponseEntity<UploadCheckResult> checkFile(
            @RequestParam String fileMd5,
            @RequestParam String fileName,
            @RequestParam Long fileSize) {
        
        UploadCheckResult result = chunkUploadService.checkFile(fileMd5, fileName, fileSize);
        return ResponseEntity.ok(result);
    }

    /**
     * 初始化分片上传
     */
    @PostMapping("/init")
    public ResponseEntity<InitUploadResult> initUpload(
            @RequestBody InitUploadRequest request) {
        
        InitUploadResult result = chunkUploadService.initUpload(request);
        return ResponseEntity.ok(result);
    }

    /**
     * 上传单个分片
     */
    @PostMapping("/chunk")
    public ResponseEntity<ChunkUploadResult> uploadChunk(
            @RequestParam String uploadId,
            @RequestParam Integer chunkNumber,
            @RequestParam MultipartFile chunk) {
        
        ChunkUploadResult result = chunkUploadService.uploadChunk(
            uploadId, chunkNumber, chunk);
        return ResponseEntity.ok(result);
    }

    /**
     * 合并分片
     */
    @PostMapping("/merge")
    public ResponseEntity<MergeResult> mergeChunks(
            @RequestBody MergeRequest request) {
        
        MergeResult result = chunkUploadService.mergeChunks(request);
        return ResponseEntity.ok(result);
    }
}
```

### 2. 分片上传服务实现

```java
@Service
@RequiredArgsConstructor
@Slf4j
public class ChunkUploadService {
    private MinioClient minioClient;
    private RedisTemplate<String, Object> redisTemplate;
    
    private static final String FILE_MD5_KEY = "minio:file:md5:";
    private static final String UPLOAD_CHUNKS_KEY = "minio:upload:chunks:";

    /**
     * 检查文件 - 秒传判断
     */
    public UploadCheckResult checkFile(String fileMd5, String fileName, Long fileSize) {
        // 1. 检查文件是否已存在(秒传)
        String existingUrl = (String) redisTemplate.opsForValue().get(FILE_MD5_KEY + fileMd5);
        if (existingUrl != null) {
            return UploadCheckResult.builder()
                .exists(true)
                .url(existingUrl)
                .message("文件已存在，秒传成功")
                .build();
        }
        
        // 2. 检查是否有未完成的上传
        Set<Integer> uploadedChunks = getUploadedChunks(fileMd5);
        
        return UploadCheckResult.builder()
            .exists(false)
            .uploadedChunks(uploadedChunks)
            .message(uploadedChunks.isEmpty() ? "新文件" : "断点续传")
            .build();
    }

    /**
     * 初始化分片上传
     */
    public InitUploadResult initUpload(InitUploadRequest request) {
        try {
            String objectName = buildFilePath(request.getPrefix(), request.getFileName());
            
            CreateMultipartUploadResponse response = minioClient.createMultipartUpload(
                CreateMultipartUploadArgs.builder()
                    .bucket(minioConfig.getBucket())
                    .object(objectName)
                    .contentType(request.getContentType())
                    .build()
            );
            
            String uploadId = response.result().uploadId();
            
            // 保存上传信息
            saveUploadInfo(request.getFileMd5(), uploadId, objectName);
            
            return InitUploadResult.builder()
                .uploadId(uploadId)
                .objectName(objectName)
                .chunkSize(PART_SIZE)
                .build();
                
        } catch (Exception e) {
            throw new RuntimeException("初始化上传失败", e);
        }
    }

    /**
     * 上传分片
     */
    public ChunkUploadResult uploadChunk(String uploadId, Integer chunkNumber, 
                                          MultipartFile chunk) {
        try {
            UploadInfo uploadInfo = getUploadInfo(uploadId);
            
            UploadPartResponse response = minioClient.uploadPart(
                UploadPartArgs.builder()
                    .bucket(minioConfig.getBucket())
                    .object(uploadInfo.getObjectName())
                    .uploadId(uploadId)
                    .partNumber(chunkNumber)
                    .stream(chunk.getInputStream(), chunk.getSize(), -1)
                    .build()
            );
            
            // 记录已上传的分片
            recordUploadedChunk(uploadInfo.getFileMd5(), chunkNumber, response.etag());
            
            return ChunkUploadResult.builder()
                .chunkNumber(chunkNumber)
                .etag(response.etag())
                .success(true)
                .build();
                
        } catch (Exception e) {
            throw new RuntimeException("分片上传失败", e);
        }
    }

    /**
     * 合并分片
     */
    public MergeResult mergeChunks(MergeRequest request) {
        try {
            UploadInfo uploadInfo = getUploadInfo(request.getUploadId());
            Map<Integer, String> chunkEtags = getChunkEtags(uploadInfo.getFileMd5());
            
            Part[] parts = chunkEtags.entrySet().stream()
                .sorted(Map.Entry.comparingByKey())
                .map(e -> new Part(e.getKey(), e.getValue()))
                .toArray(Part[]::new);
            
            minioClient.completeMultipartUpload(
                CompleteMultipartUploadArgs.builder()
                    .bucket(minioConfig.getBucket())
                    .object(uploadInfo.getObjectName())
                    .uploadId(request.getUploadId())
                    .parts(parts)
                    .build()
            );
            
            String url = buildAccessUrl(uploadInfo.getObjectName());
            
            // 保存文件MD5映射(用于秒传)
            redisTemplate.opsForValue().set(
                FILE_MD5_KEY + uploadInfo.getFileMd5(), 
                url,
                30, TimeUnit.DAYS
            );
            
            // 清理临时数据
            cleanupUploadData(request.getUploadId(), uploadInfo.getFileMd5());
            
            return MergeResult.builder()
                .success(true)
                .url(url)
                .build();
                
        } catch (Exception e) {
            throw new RuntimeException("合并分片失败", e);
        }
    }
}
```

### 3. 前端分片上传示例

```java
class ChunkUploader {
    constructor(options) {
        this.chunkSize = options.chunkSize || 5 * 1024 * 1024; // 5MB
        this.concurrent = options.concurrent || 3; // 并行数
        this.onProgress = options.onProgress || (() => {});
    }

    async upload(file) {
        // 1. 计算文件MD5
        const fileMd5 = await this.calculateMD5(file);
        
        // 2. 检查是否可以秒传
        const checkResult = await this.checkFile(fileMd5, file.name, file.size);
        if (checkResult.exists) {
            this.onProgress(100);
            return checkResult.url;
        }
        
        // 3. 初始化上传
        const initResult = await this.initUpload(fileMd5, file.name, file.size);
        const { uploadId, chunkSize } = initResult;
        
        // 4. 分片上传
        const chunks = this.createChunks(file, chunkSize);
        const uploadedChunks = new Set(checkResult.uploadedChunks || []);
        
        await this.uploadChunks(uploadId, chunks, uploadedChunks);
        
        // 5. 合并分片
        const mergeResult = await this.mergeChunks(uploadId, fileMd5);
        return mergeResult.url;
    }

    createChunks(file, chunkSize) {
        const chunks = [];
        let start = 0;
        let index = 1;
        
        while (start < file.size) {
            const end = Math.min(start + chunkSize, file.size);
            chunks.push({
                index: index++,
                blob: file.slice(start, end),
                start,
                end
            });
            start = end;
        }
        
        return chunks;
    }

    async uploadChunks(uploadId, chunks, uploadedChunks) {
        const pendingChunks = chunks.filter(c => !uploadedChunks.has(c.index));
        const total = chunks.length;
        let completed = uploadedChunks.size;

        // 控制并发上传
        const pool = [];
        
        for (const chunk of pendingChunks) {
            const promise = this.uploadChunk(uploadId, chunk)
                .then(() => {
                    completed++;
                    this.onProgress((completed / total) * 100);
                });
            
            pool.push(promise);
            
            if (pool.length >= this.concurrent) {
                await Promise.race(pool);
                pool.splice(pool.findIndex(p => p.status === 'fulfilled'), 1);
            }
        }
        
        await Promise.all(pool);
    }
}
```


